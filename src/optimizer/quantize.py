"""
ëª¨ë¸ ì–‘ìí™” ìŠ¤í¬ë¦½íŠ¸ (AWQ / GPTQ)
í•™ìŠµëœ ëª¨ë¸ì„ AWQ ë˜ëŠ” GPTQë¡œ ì–‘ìí™”í•˜ì—¬ ì¶”ë¡  ì†ë„ í–¥ìƒ ë° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ
"""

import argparse
import logging
import os
from pathlib import Path
from typing import List, Optional

import torch
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer

from src.utils.log import logger


class QuantizationConfig:
    """ì–‘ìí™” ì„¤ì •"""
    
    def __init__(
        self,
        model_path: str,
        output_dir: str,
        quantization_type: str = "awq",
        bits: int = 4,
        group_size: int = 128,
        calibration_samples: int = 512,
        dataset_name: str = "Devocean-06/Spam_QA-Corpus",
        dataset_split: str = "train",
        max_seq_length: int = 1500,
    ):
        self.model_path = model_path
        self.output_dir = output_dir
        self.quantization_type = quantization_type.lower()
        self.bits = bits
        self.group_size = group_size
        self.calibration_samples = calibration_samples
        self.dataset_name = dataset_name
        self.dataset_split = dataset_split
        self.max_seq_length = max_seq_length
        
        # ì–‘ìí™” íƒ€ì… ê²€ì¦
        if self.quantization_type not in ["awq", "gptq"]:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì–‘ìí™” íƒ€ì…ì…ë‹ˆë‹¤: {quantization_type}")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        Path(self.output_dir).mkdir(parents=True, exist_ok=True)


class CalibrationDataProcessor:
    """Calibration ë°ì´í„° ì „ì²˜ë¦¬ê¸°"""
    
    def __init__(self, tokenizer, config: QuantizationConfig):
        self.tokenizer = tokenizer
        self.config = config
    
    def _build_prompt(self, sample: dict) -> str:
        """Alpaca í¬ë§·(instruction/input/output)ì„ í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜"""
        instruction = sample.get("instruction", "").strip()
        input_text = sample.get("input", "").strip()
        
        if input_text:
            # Chat í…œí”Œë¦¿ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            prompt = f"{instruction}\n\nInput: {input_text}"
        else:
            prompt = instruction
        
        return prompt
    
    def prepare_calibration_data(self) -> List[str]:
        """
        Calibration ë°ì´í„° ì¤€ë¹„
        - Alpaca í˜•ì‹ ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ì—¬ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        """
        logger.info(f"ğŸ“Š Calibration ë°ì´í„° ë¡œë“œ: {self.config.dataset_name}")
        
        # ë°ì´í„°ì…‹ ë¡œë“œ
        dataset = load_dataset(
            self.config.dataset_name, 
            split=self.config.dataset_split
        )
        
        # ìƒ˜í”Œ ìˆ˜ ì œí•œ
        if len(dataset) > self.config.calibration_samples:
            dataset = dataset.shuffle(seed=42).select(range(self.config.calibration_samples))
        
        # í…ìŠ¤íŠ¸ ë³€í™˜
        calibration_texts = []
        for sample in dataset:
            prompt = self._build_prompt(sample)
            
            # Tokenizerì˜ chat templateì´ ìˆìœ¼ë©´ ì‚¬ìš©
            if hasattr(self.tokenizer, 'chat_template') and self.tokenizer.chat_template:
                messages = [{"role": "user", "content": prompt}]
                text = self.tokenizer.apply_chat_template(
                    messages, 
                    tokenize=False, 
                    add_generation_prompt=True
                )
            else:
                text = prompt
            
            calibration_texts.append(text)
        
        logger.info(f"âœ… Calibration ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {len(calibration_texts)}ê°œ ìƒ˜í”Œ")
        return calibration_texts
    
    def prepare_calibration_tokens(self) -> List[torch.Tensor]:
        """
        Calibration ë°ì´í„°ë¥¼ í† í¬ë‚˜ì´ì¦ˆ
        """
        texts = self.prepare_calibration_data()
        
        logger.info(f"ğŸ”„ Calibration ë°ì´í„° í† í¬ë‚˜ì´ì¦ˆ ì¤‘...")
        tokenized_data = []
        
        for text in texts:
            tokens = self.tokenizer(
                text,
                return_tensors="pt",
                max_length=self.config.max_seq_length,
                truncation=True,
                padding=False,
            )
            tokenized_data.append(tokens["input_ids"])
        
        logger.info(f"âœ… í† í¬ë‚˜ì´ì¦ˆ ì™„ë£Œ: {len(tokenized_data)}ê°œ ìƒ˜í”Œ")
        return tokenized_data


class AWQQuantizer:
    """AWQ ì–‘ìí™”ê¸°"""
    
    def __init__(self, config: QuantizationConfig):
        self.config = config
    
    def quantize(self):
        """AWQ ì–‘ìí™” ìˆ˜í–‰"""
        try:
            from awq import AutoAWQForCausalLM
        except ImportError:
            raise ImportError(
                "AWQ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. "
                "ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”: pip install autoawq"
            )
        
        logger.info("ğŸš€ AWQ ì–‘ìí™” ì‹œì‘")
        logger.info(f"   - ëª¨ë¸: {self.config.model_path}")
        logger.info(f"   - Bits: {self.config.bits}")
        logger.info(f"   - Group Size: {self.config.group_size}")
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        logger.info("ğŸ“š í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...")
        tokenizer = AutoTokenizer.from_pretrained(self.config.model_path)
        
        # Calibration ë°ì´í„° ì¤€ë¹„
        data_processor = CalibrationDataProcessor(tokenizer, self.config)
        calibration_texts = data_processor.prepare_calibration_data()
        
        # AWQ ëª¨ë¸ ë¡œë“œ
        logger.info("ğŸ”§ AWQ ëª¨ë¸ ë¡œë“œ ì¤‘...")
        model = AutoAWQForCausalLM.from_pretrained(
            self.config.model_path,
            device_map="auto",
        )
        
        # ì–‘ìí™” ì„¤ì •
        quant_config = {
            "zero_point": True,
            "q_group_size": self.config.group_size,
            "w_bit": self.config.bits,
            "version": "GEMM"
        }
        
        # ì–‘ìí™” ìˆ˜í–‰
        logger.info("âš¡ AWQ ì–‘ìí™” ìˆ˜í–‰ ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        model.quantize(
            tokenizer,
            quant_config=quant_config,
            calib_data=calibration_texts,
        )
        
        # ì–‘ìí™”ëœ ëª¨ë¸ ì €ì¥
        logger.info(f"ğŸ’¾ ì–‘ìí™”ëœ ëª¨ë¸ ì €ì¥: {self.config.output_dir}")
        model.save_quantized(self.config.output_dir)
        tokenizer.save_pretrained(self.config.output_dir)
        
        logger.info("âœ… AWQ ì–‘ìí™” ì™„ë£Œ!")


class GPTQQuantizer:
    """GPTQ ì–‘ìí™”ê¸°"""
    
    def __init__(self, config: QuantizationConfig):
        self.config = config
    
    def quantize(self):
        """GPTQ ì–‘ìí™” ìˆ˜í–‰"""
        try:
            from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig
        except ImportError:
            raise ImportError(
                "GPTQ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. "
                "ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”: pip install auto-gptq"
            )
        
        logger.info("ğŸš€ GPTQ ì–‘ìí™” ì‹œì‘")
        logger.info(f"   - ëª¨ë¸: {self.config.model_path}")
        logger.info(f"   - Bits: {self.config.bits}")
        logger.info(f"   - Group Size: {self.config.group_size}")
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        logger.info("ğŸ“š í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...")
        tokenizer = AutoTokenizer.from_pretrained(
            self.config.model_path,
            use_fast=True,
        )
        
        # Calibration ë°ì´í„° ì¤€ë¹„
        data_processor = CalibrationDataProcessor(tokenizer, self.config)
        calibration_texts = data_processor.prepare_calibration_data()
        
        # GPTQ ì–‘ìí™” ì„¤ì •
        quantize_config = BaseQuantizeConfig(
            bits=self.config.bits,
            group_size=self.config.group_size,
            desc_act=False,  # activationì— ëŒ€í•œ ì •ë ¬ ë¹„í™œì„±í™” (ì†ë„ í–¥ìƒ)
            damp_percent=0.01,
        )
        
        # GPTQ ëª¨ë¸ ë¡œë“œ
        logger.info("ğŸ”§ GPTQ ëª¨ë¸ ë¡œë“œ ì¤‘...")
        model = AutoGPTQForCausalLM.from_pretrained(
            self.config.model_path,
            quantize_config=quantize_config,
            device_map="auto",
        )
        
        # ì–‘ìí™”ìš© ë°ì´í„°ì…‹ ì¤€ë¹„
        logger.info("ğŸ“Š Calibration ë°ì´í„°ì…‹ êµ¬ì„± ì¤‘...")
        
        # GPTQëŠ” ë¦¬ìŠ¤íŠ¸ì˜ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¥¼ ìš”êµ¬
        calibration_dataset = []
        for text in calibration_texts[:self.config.calibration_samples]:
            tokens = tokenizer(
                text,
                return_tensors="pt",
                max_length=self.config.max_seq_length,
                truncation=True,
            )
            calibration_dataset.append(tokens)
        
        # ì–‘ìí™” ìˆ˜í–‰
        logger.info("âš¡ GPTQ ì–‘ìí™” ìˆ˜í–‰ ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        model.quantize(
            calibration_dataset,
            batch_size=1,
        )
        
        # ì–‘ìí™”ëœ ëª¨ë¸ ì €ì¥
        logger.info(f"ğŸ’¾ ì–‘ìí™”ëœ ëª¨ë¸ ì €ì¥: {self.config.output_dir}")
        model.save_quantized(self.config.output_dir)
        tokenizer.save_pretrained(self.config.output_dir)
        
        logger.info("âœ… GPTQ ì–‘ìí™” ì™„ë£Œ!")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="ëª¨ë¸ ì–‘ìí™” (AWQ / GPTQ)")
    
    # í•„ìˆ˜ ì¸ì
    parser.add_argument(
        "--model-path",
        type=str,
        required=True,
        help="ì–‘ìí™”í•  ëª¨ë¸ ê²½ë¡œ (merged ëª¨ë¸ ë˜ëŠ” base ëª¨ë¸)"
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        required=True,
        help="ì–‘ìí™”ëœ ëª¨ë¸ ì €ì¥ ê²½ë¡œ"
    )
    
    # ì–‘ìí™” ì„¤ì •
    parser.add_argument(
        "--quantization-type",
        type=str,
        default="awq",
        choices=["awq", "gptq"],
        help="ì–‘ìí™” íƒ€ì… (ê¸°ë³¸ê°’: awq)"
    )
    parser.add_argument(
        "--bits",
        type=int,
        default=4,
        choices=[2, 3, 4, 8],
        help="ì–‘ìí™” ë¹„íŠ¸ ìˆ˜ (ê¸°ë³¸ê°’: 4)"
    )
    parser.add_argument(
        "--group-size",
        type=int,
        default=128,
        help="ê·¸ë£¹ ì‚¬ì´ì¦ˆ (ê¸°ë³¸ê°’: 128)"
    )
    
    # Calibration ë°ì´í„° ì„¤ì •
    parser.add_argument(
        "--calibration-samples",
        type=int,
        default=512,
        help="Calibration ìƒ˜í”Œ ìˆ˜ (ê¸°ë³¸ê°’: 512)"
    )
    parser.add_argument(
        "--dataset-name",
        type=str,
        default="Devocean-06/Spam_QA-Corpus",
        help="Calibration ë°ì´í„°ì…‹ ì´ë¦„"
    )
    parser.add_argument(
        "--dataset-split",
        type=str,
        default="train",
        help="ë°ì´í„°ì…‹ ìŠ¤í”Œë¦¿ (ê¸°ë³¸ê°’: train)"
    )
    parser.add_argument(
        "--max-seq-length",
        type=int,
        default=1500,
        help="ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ (ê¸°ë³¸ê°’: 1500)"
    )
    
    args = parser.parse_args()
    
    # ì–‘ìí™” ì„¤ì • ìƒì„±
    config = QuantizationConfig(
        model_path=args.model_path,
        output_dir=args.output_dir,
        quantization_type=args.quantization_type,
        bits=args.bits,
        group_size=args.group_size,
        calibration_samples=args.calibration_samples,
        dataset_name=args.dataset_name,
        dataset_split=args.dataset_split,
        max_seq_length=args.max_seq_length,
    )
    
    # ì–‘ìí™” ìˆ˜í–‰
    try:
        if config.quantization_type == "awq":
            quantizer = AWQQuantizer(config)
        else:  # gptq
            quantizer = GPTQQuantizer(config)
        
        quantizer.quantize()
        
        logger.info("=" * 80)
        logger.info("ğŸ‰ ì–‘ìí™” ì™„ë£Œ!")
        logger.info(f"   - ì–‘ìí™” íƒ€ì…: {config.quantization_type.upper()}")
        logger.info(f"   - ì €ì¥ ê²½ë¡œ: {config.output_dir}")
        logger.info("=" * 80)
        
    except Exception as e:
        logger.error(f"âŒ ì–‘ìí™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise


if __name__ == "__main__":
    main()

