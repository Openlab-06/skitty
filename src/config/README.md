# âš™ï¸ Configuration Module

Skitty í”„ë¡œì íŠ¸ì˜ ëª¨ë“  ì„¤ì •ì„ ê´€ë¦¬í•˜ëŠ” ëª¨ë“ˆì…ë‹ˆë‹¤.

# íŒŒì¸íŠœë‹ìš© íŒ¨í‚¤ì§€ ì„¤ì¹˜
uv pip install axolotl
uv pip install torch torchvision torchaudio
uv pip install --no-build-isolation flash-attn
uv pip install vllm

## ğŸ“ ì„¤ì • íŒŒì¼ êµ¬ì„±

| íŒŒì¼ | ëª©ì  | ì„¤ëª… |
|------|------|------|
| `env_config.py` | ğŸŒ í™˜ê²½ ë³€ìˆ˜ ê´€ë¦¬ | API í‚¤, ëª¨ë¸ëª… ë“± ë¯¼ê°ì •ë³´ |
| `data_config.py` | ğŸ“Š ë°ì´í„° ì²˜ë¦¬ ì„¤ì • | ì¤‘ë³µì œê±°, ì •ê·œí™” íŒŒë¼ë¯¸í„° |
| `gemma3.yaml` | ğŸ¤– Axolotl í•™ìŠµ ì„¤ì • | ëª¨ë¸ í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„° |

## ğŸŒ í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (`env_config.py`)

### ì„¤ì • í´ë˜ìŠ¤
```python
class ProjectConfig(BaseSettings):
    # Gemini API ì„¤ì •
    GEMINI_API_KEY: str           # í•„ìˆ˜: Gemini API í‚¤
    GEMINI_MODEL_ARGU: str        # ë°ì´í„° ì¦ê°•ìš© ëª¨ë¸
    GEMINI_MODEL_FILTER: str      # ë°ì´í„° í•„í„°ë§ìš© ëª¨ë¸
    
    model_config = ConfigDict(
        env_file=".env",
        env_file_encoding="utf-8",
        extra="ignore"
    )
```

### ì‚¬ìš©ë²•
```python
from src.config.env_config import get_config

# ì‹±ê¸€í†¤ íŒ¨í„´ìœ¼ë¡œ ì„¤ì • ë¡œë“œ
config = get_config()
api_key = config.GEMINI_API_KEY
model_name = config.GEMINI_MODEL_ARGU
```

### í™˜ê²½ íŒŒì¼ ì„¤ì • (`.env`)
í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— `.env` íŒŒì¼ì„ ìƒì„±í•˜ì„¸ìš”:

```bash
# .env íŒŒì¼ ì˜ˆì‹œ
GEMINI_API_KEY=your_gemini_api_key_here
GEMINI_MODEL_ARGU=gemini-2.5-flash
GEMINI_MODEL_FILTER=gemini-2.5-flash-lite
OPENAI_API_KEY=your_gpt_api_key_here
OPENAI_MODEL=gpt-4.1-mini # 5ì˜ ê²½ìš° reasoning_effortë¥¼ lowë¡œ ì£¼ì–´ë„ ìƒëŒ€ì ìœ¼ë¡œ ì¶”ë¡  ì‹œê°„ì´ ê¸¸ì–´ gpt-4.1-mini ì±„íƒ
```

**âš ï¸ ë³´ì•ˆ ì£¼ì˜ì‚¬í•­:**
- `.env` íŒŒì¼ì€ `.gitignore`ì— í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤
- API í‚¤ëŠ” ì ˆëŒ€ ì½”ë“œì— í•˜ë“œì½”ë”©í•˜ì§€ ë§ˆì„¸ìš”
- í™˜ê²½ë³„ë¡œ ë‹¤ë¥¸ `.env` íŒŒì¼ ì‚¬ìš© ê¶Œì¥

## ğŸ“Š ë°ì´í„° ì²˜ë¦¬ ì„¤ì • (`data_config.py`)

### ì¤‘ë³µì œê±° ì„¤ì •
```python
class DeduplicationConfig:
    # ê¸°ë³¸ ì„¤ì •
    TEXT_COL = "CN"                      # í…ìŠ¤íŠ¸ ì»¬ëŸ¼ëª…
    ID_COL = "_rowid_"                   # ê³ ìœ  ID ì»¬ëŸ¼ëª…
    
    # Simhash íŒŒë¼ë¯¸í„°
    SIMHASH_K = 3                        # í•´ë° ê±°ë¦¬ ì„ê³„ê°’ (0-64)
    NGRAM_N = 2                          # Character n-gram í¬ê¸°
    
    # íŒŒì¼ ì €ì¥ ì„¤ì •
    COMPRESSION = "lz4"                  # Parquet ì••ì¶• ë°©ì‹
    DEFAULT_UNIQUE_OUTPUT = "./src/data/deduplicated_result.parquet"
    DEFAULT_DUPS_OUTPUT = "./src/data/duplicate_analysis.parquet"
    
    # ì •ê·œí™” íŒ¨í„´
    PHONE_PATTERN = r"0\d{1,2}-?\d{3,4}-?\d{4}"
    URL_PATTERN = r"https?://[^\s]+"
    NUM_PATTERN = r"\d{1,3}(,\d{3})+"
```

### íŒŒë¼ë¯¸í„° ì„¤ëª…

#### Simhash ì„¤ì •
- **SIMHASH_K**: í•´ë° ê±°ë¦¬ ì„ê³„ê°’
  - ë‚®ì„ìˆ˜ë¡ ì—„ê²©í•œ ì¤‘ë³µ íŒì • (0: ì™„ì „ ì¼ì¹˜)
  - ë†’ì„ìˆ˜ë¡ ê´€ëŒ€í•œ ì¤‘ë³µ íŒì • (64: ëª¨ë“  ë¬¸ì„œ ì¤‘ë³µ)
  - **ê¶Œì¥ê°’**: 3-5 (ìŠ¤íŒ¸ ë¬¸ì íŠ¹ì„±ìƒ)

- **NGRAM_N**: Character n-gram í¬ê¸°
  - 1: ë¬¸ì ë‹¨ìœ„ (ë„ˆë¬´ ë¯¼ê°)
  - 2: 2ë¬¸ì ì¡°í•© (ê¶Œì¥)
  - 3+: ê¸´ íŒ¨í„´ (ëœ ë¯¼ê°)

#### ì••ì¶• ì˜µì…˜
- **lz4**: ë¹ ë¥¸ ì••ì¶•/í•´ì œ (ê¶Œì¥)
- **snappy**: ì••ì¶•ë¥ ê³¼ ì†ë„ì˜ ê· í˜•
- **gzip**: ë†’ì€ ì••ì¶•ë¥ , ëŠë¦° ì†ë„

### ì‚¬ìš©ë²•
```python
from src.config.data_config import DeduplicationConfig

# ì„¤ì •ê°’ ì‚¬ìš©
text_column = DeduplicationConfig.TEXT_COL
hamming_distance = DeduplicationConfig.SIMHASH_K

# CLIì—ì„œ ì˜¤ë²„ë¼ì´ë“œ
python -m src.data.data_processing \
    --input data.csv \
    --text_col "message" \
    --k 5
```

## ğŸ¤– Axolotl í•™ìŠµ ì„¤ì • (`gemma3.yaml`)

### ê¸°ë³¸ ëª¨ë¸ ì„¤ì •
```yaml
# ë² ì´ìŠ¤ ëª¨ë¸
base_model: google/gemma-3-4b-it
load_in_4bit: true

# ë°ì´í„°ì…‹ ì„¤ì •
datasets:
  - path: ./src/data/final_spam.csv
    type: alpaca
    field_instruction: instruction
    field_input: input
    field_output: output

# ì¶œë ¥ ë””ë ‰í† ë¦¬
output_dir: ./outputs/gemma3
```

### LoRA ì„¤ì •
```yaml
# ì–´ëŒ‘í„° ì„¤ì •
adapter: qlora
lora_r: 32              # LoRA rank (ë‚®ì„ìˆ˜ë¡ ì••ì¶•ë¥  ë†’ìŒ)
lora_alpha: 16          # LoRA scaling factor
lora_dropout: 0.05      # Dropout ë¹„ìœ¨
lora_target_modules:    # LoRA ì ìš© ëª¨ë“ˆ
  - q_proj
  - v_proj
  - k_proj
  - o_proj
  - gate_proj
  - up_proj
  - down_proj
```

### í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°
```yaml
# ì‹œí€€ìŠ¤ ì„¤ì •
sequence_len: 512       # ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
sample_packing: true    # ë°°ì¹˜ íŒ¨í‚¹ í™œì„±í™”

# í•™ìŠµ ì„¤ì •
micro_batch_size: 2     # ë§ˆì´í¬ë¡œ ë°°ì¹˜ í¬ê¸°
gradient_accumulation_steps: 1
num_epochs: 3           # í•™ìŠµ ì—í­ ìˆ˜
optimizer: adamw_bnb_8bit
lr_scheduler: cosine
learning_rate: 0.0002   # í•™ìŠµë¥ 

# ì •ê·œí™”
weight_decay: 0.0
warmup_steps: 10
evals_per_epoch: 4
saves_per_epoch: 2
```

### WandB ì—°ë™ ì„¤ì •
```yaml
# ì‹¤í—˜ ì¶”ì 
wandb_project: skitty-spam-filter
wandb_entity: your_wandb_username
wandb_watch: gradients
wandb_name: gemma3-spam-{timestamp}
wandb_log_model: checkpoint

# ë¡œê¹… ì„¤ì •
logging_steps: 1
log_sweep_parameters: true
```

### ê³ ê¸‰ ì„¤ì •
```yaml
# ë©”ëª¨ë¦¬ ìµœì í™”
fp16: true
bf16: false
tf32: false
gradient_checkpointing: true

# DDP ì„¤ì • (ë©€í‹° GPU)
ddp_find_unused_parameters: true
dataloader_pin_memory: false

# ì•ˆì „ ì„¤ì •
strict: false           # ì—„ê²© ëª¨ë“œ ë¹„í™œì„±í™”
resize_token_embeddings_to_32x: true
```

## ğŸ”§ ì„¤ì • íŒŒì¼ ì‚¬ìš©ë²•

### 1. ê¸°ë³¸ í•™ìŠµ ì‹¤í–‰
```bash
# ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ í•™ìŠµ
./run_train.sh

# íŠ¹ì • ì„¤ì • íŒŒì¼ ì‚¬ìš©
./run_train.sh custom_config.yaml
```

### 2. ì„¤ì • ì˜¤ë²„ë¼ì´ë“œ
```bash
# ë°°ì¹˜ ì‚¬ì´ì¦ˆ ë³€ê²½
./run_train.sh --batch-size 4

# í•™ìŠµë¥  ë³€ê²½
./run_train.sh --learning-rate 0.0001

# GPU ê°œìˆ˜ ì§€ì •
./run_train.sh --gpus 2
```

### 3. ì„¤ì • ê²€ì¦
```bash
# ì„¤ì • íŒŒì¼ ê²€ì¦ë§Œ ì‹¤í–‰
./run_train.sh --validate-only

# ë“œë¼ì´ ëŸ° (ëª…ë ¹ì–´ë§Œ ì¶œë ¥)
./run_train.sh --dry-run
```

## ğŸ“Š í™˜ê²½ë³„ ì„¤ì • ê´€ë¦¬

### ê°œë°œ í™˜ê²½ (Development)
```yaml
# dev_config.yaml
num_epochs: 1
micro_batch_size: 1
eval_steps: 10
save_steps: 100
wandb_mode: disabled    # WandB ë¹„í™œì„±í™”
```

### í”„ë¡œë•ì…˜ í™˜ê²½ (Production)
```yaml
# prod_config.yaml
num_epochs: 5
micro_batch_size: 4
eval_steps: 100
save_steps: 500
wandb_mode: online      # WandB í™œì„±í™”
early_stopping_patience: 3
```

### ì‹¤í—˜ í™˜ê²½ (Experiment)
```yaml
# exp_config.yaml
num_epochs: 10
micro_batch_size: 2
learning_rate: 0.0001
lora_r: 64             # ë” í° LoRA rank
wandb_project: skitty-experiments
```

## ğŸš¨ ì£¼ì˜ì‚¬í•­ ë° ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤

### 1. ë³´ì•ˆ ê´€ë¦¬
```bash
# í™˜ê²½ ë³€ìˆ˜ í™•ì¸
echo $GEMINI_API_KEY

# API í‚¤ ìœ íš¨ì„± ê²€ì¦
curl -H "Authorization: Bearer $GEMINI_API_KEY" \
     https://generativelanguage.googleapis.com/v1beta/models
```

### 2. ë©”ëª¨ë¦¬ ê´€ë¦¬
- **4GB+ GPU**: `micro_batch_size: 2`
- **8GB+ GPU**: `micro_batch_size: 4`
- **16GB+ GPU**: `micro_batch_size: 8`

### 3. í•™ìŠµ ì•ˆì •ì„±
```yaml
# í•™ìŠµ ì¤‘ë‹¨ ì‹œ ì¬ê°œ ê°€ëŠ¥í•œ ì„¤ì •
resume_from_checkpoint: auto
save_safetensors: true
auto_resume_from_checkpoints: true
```

### 4. ë””ë²„ê¹… ì„¤ì •
```yaml
# ë””ë²„ê·¸ ëª¨ë“œ
debug: true
max_steps: 100          # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
eval_steps: 10
logging_steps: 1
wandb_mode: disabled
```

## ğŸ“š ì°¸ê³  ìë£Œ

### Axolotl ê³µì‹ ë¬¸ì„œ
- [Axolotl GitHub](https://github.com/OpenAccess-AI-Collective/axolotl)
- [LoRA ì„¤ì • ê°€ì´ë“œ](https://github.com/OpenAccess-AI-Collective/axolotl/blob/main/docs/config.md)
- [ë°ì´í„°ì…‹ í˜•ì‹](https://github.com/OpenAccess-AI-Collective/axolotl/blob/main/docs/dataset-formats.md)

### ëª¨ë¸ ê´€ë ¨
- [Gemma-3 ëª¨ë¸ ì¹´ë“œ](https://huggingface.co/google/gemma-3-4b-it)
- [LoRA ë…¼ë¬¸](https://arxiv.org/abs/2106.09685)
- [QLoRA ë…¼ë¬¸](https://arxiv.org/abs/2305.14314)

### ëª¨ë‹ˆí„°ë§
- [WandB ì„¤ì • ê°€ì´ë“œ](https://docs.wandb.ai/guides/integrations/axolotl)
- [í…ì„œë³´ë“œ ì—°ë™](https://pytorch.org/docs/stable/tensorboard.html)
