# ğŸ† Spam XAI Evaluation System

ìŠ¤íŒ¸ íƒì§€ ì„¤ëª… ê°€ëŠ¥ì„± AI (Explainable AI) í‰ê°€ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

## ğŸ“Š í‰ê°€ ë©”íŠ¸ë¦­

ì„¸ ê°€ì§€ ë©”íŠ¸ë¦­ì„ ì¡°í•©í•˜ì—¬ ìŠ¤íŒ¸ íƒì§€ ì„¤ëª…ì˜ í’ˆì§ˆì„ ì¢…í•© í‰ê°€í•©ë‹ˆë‹¤:

### 1ï¸âƒ£ BLEU Score (ê°€ì¤‘ì¹˜: 25%)

**ëª©ì **: ì–´íœ˜ ìˆ˜ì¤€ì˜ ì¼ì¹˜ë„ ì¸¡ì •

- **ë²”ìœ„**: 0.0 ~ 1.0 (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
- **ì„¤ëª…**: ìƒì„±ëœ ì„¤ëª…ì´ ê¸°ì¤€ ì„¤ëª…ê³¼ ì–¼ë§ˆë‚˜ ë§ì€ ë‹¨ì–´ë¥¼ ê³µìœ í•˜ëŠ”ì§€ ì¸¡ì •
- **íŠ¹ì§•**: 
  - n-gram ê¸°ë°˜ì˜ ì •í™•í•œ ì¼ì¹˜ë„ ê³„ì‚°
  - ê°™ì€ ì˜ë¯¸ë¼ë„ ë‹¤ë¥¸ ë‹¨ì–´ ì‚¬ìš© ì‹œ ì ìˆ˜ ê°ì†Œ

### 2ï¸âƒ£ Semantic Similarity (ê°€ì¤‘ì¹˜: 35%)

**ëª©ì **: ì˜ë¯¸ë¡ ì  ìœ ì‚¬ì„± ì¸¡ì •

- **ë²”ìœ„**: 0.0 ~ 1.0 (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
- **ì„¤ëª…**: ìƒì„±ëœ ì„¤ëª…ì˜ ì˜ë¯¸ê°€ ê¸°ì¤€ ì„¤ëª…ê³¼ ì–¼ë§ˆë‚˜ ì¼ì¹˜í•˜ëŠ”ì§€ ì¸¡ì •
- **ê¸°ìˆ **: Cross-Encoder (qnli-distilroberta-base) ê¸°ë°˜
- **íŠ¹ì§•**:
  - ë¬¸ë§¥ì„ ê³ ë ¤í•œ ì˜ë¯¸ ë¹„êµ
  - ë‹¤ë¥¸ ë‹¨ì–´ì§€ë§Œ ê°™ì€ ì˜ë¯¸ë©´ ë†’ì€ ì ìˆ˜
  - ì„¤ëª…ì˜ í•µì‹¬ ë‚´ìš©ì´ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸

### 3ï¸âƒ£ LLM Judge Score (ê°€ì¤‘ì¹˜: 40%)

**ëª©ì **: ì„¤ëª… í’ˆì§ˆì˜ ì¢…í•© í‰ê°€

- **ë²”ìœ„**: 0.0 ~ 1.0 (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
- **íŒì‚¬ ëª¨ë¸**: GPT-4o (SOTA ëª¨ë¸)
- **í‰ê°€ ê¸°ì¤€**:
  - **ì •í™•ì„± (Accuracy)**: ì„¤ëª…ì´ ìŠ¤íŒ¸ íŒì •ì˜ ì‹¤ì œ ì´ìœ ì™€ ì¼ì¹˜í•˜ëŠ”ê°€?
  - **ì™„ì „ì„± (Completeness)**: ëª¨ë“  ìŠ¤íŒ¸ íŒì • ê·¼ê±°ë¥¼ í¬í•¨í–ˆëŠ”ê°€?
  - **ëª…í™•ì„± (Clarity)**: ì„¤ëª…ì´ ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ê°€?
  - **êµ¬ì²´ì„± (Specificity)**: êµ¬ì²´ì ì¸ ê·¼ê±°ë¥¼ ì œì‹œí–ˆëŠ”ê°€?
- **íŠ¹ì§•**:
  - ì¸ê°„ ìˆ˜ì¤€ì˜ ì´í•´ë„ë¡œ í‰ê°€
  - XAI ì„¤ëª…ì˜ ì‹¤ì§ˆì  í’ˆì§ˆ íŒë‹¨

## ğŸ¯ ì¢…í•© ì ìˆ˜ (Overall Score)

```
Overall Score = BLEU Ã— 0.25 + Semantic Ã— 0.35 + LLM Judge Ã— 0.40
```

**ê°€ì¤‘ì¹˜ ê·¼ê±°**:
- **BLEU (25%)**: ê¸°ë³¸ì ì¸ ì¼ì¹˜ë„ëŠ” í•„ìš”í•˜ì§€ë§Œ ì™„ë²½í•œ ì¼ì¹˜ëŠ” ì•„ë‹˜
- **Semantic (35%)**: ì˜ë¯¸ê°€ ê°€ì¥ ì¤‘ìš”í•˜ë¯€ë¡œ ê°€ì¥ ë†’ì€ ë¹„ì¤‘
- **LLM Judge (40%)**: ìµœì¢… í’ˆì§ˆ íŒë‹¨ìœ¼ë¡œ ê°€ì¥ ë†’ì€ ë¹„ì¤‘

## ğŸš€ ì‚¬ìš© ë°©ë²•

### 1. ê¸°ë³¸ ì‚¬ìš©

```bash
./scripts/run_evaluate.sh
```

ì „ì²´ ë°ì´í„°ì…‹ìœ¼ë¡œ í‰ê°€ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.

### 2. ìƒ˜í”Œ ìˆ˜ ì œí•œ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)

```bash
./scripts/run_evaluate.sh --limit 10
```

ì²˜ìŒ 10ê°œ ìƒ˜í”Œë§Œìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤.

### 3. ìƒì„¸ ë¡œê·¸ ì¶œë ¥

```bash
./scripts/run_evaluate.sh --limit 50 --verbose
```

í‰ê°€ ì§„í–‰ ìƒí™©ì„ ìƒì„¸íˆ ì¶œë ¥í•©ë‹ˆë‹¤.

### 4. íŠ¹ì • ëª¨ë¸ í‰ê°€

```bash
./scripts/run_evaluate.sh --model "google/gemma-3-4b-it"
```

ì§€ì •í•œ ëª¨ë¸ë¡œ í‰ê°€í•©ë‹ˆë‹¤.

### 5. ë³µí•© ì˜µì…˜

```bash
./scripts/run_evaluate.sh --limit 100 --verbose --model "google/gemma-3-4b-it"
```

100ê°œ ìƒ˜í”Œì„ ìƒì„¸ ë¡œê·¸ì™€ í•¨ê»˜ í‰ê°€í•©ë‹ˆë‹¤.

## ğŸ“‹ Python ì§ì ‘ ì‹¤í–‰

```bash
# ê¸°ë³¸ ì‹¤í–‰ (ì „ì²´ ë°ì´í„°ì…‹)
python eval/evaluation.py

# 10ê°œ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸
python eval/evaluation.py --limit 10

# ìƒì„¸ ë¡œê·¸ ì¶œë ¥
python eval/evaluation.py --limit 50 --verbose true

# íŠ¹ì • ëª¨ë¸ë¡œ í‰ê°€
python eval/evaluation.py --model "google/gemma-3-4b-it"

# ëª¨ë“  ì˜µì…˜ ì ìš©
python eval/evaluation.py --limit 100 --verbose true --model "google/gemma-3-4b-it"
```

## ğŸ“Š ë¦¬ë”ë³´ë“œ ì¶œë ¥ ì˜ˆì‹œ

```
================================================================================
                 ğŸ† SPAM XAI EVALUATION LEADERBOARD ğŸ†
================================================================================

Model                BLEU         Semantic     LLM Judge    Overall
--------------------------------------------------------------------
google/gemma-3-4b-it 0.7234       0.8456       0.8102       0.8173

--------------------------------------------------------------------------------
                           ğŸ“Š Metric Details
--------------------------------------------------------------------------------

âœ“ BLEU Score (ì–´íœ˜ ì¼ì¹˜ë„): 0.7234
  - ë²”ìœ„: 0~1 (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
  - ì„¤ëª…ì´ ê¸°ì¤€ ì„¤ëª…ê³¼ ì–¼ë§ˆë‚˜ ì¼ì¹˜í•˜ëŠ”ì§€ ì¸¡ì •

âœ“ Semantic Similarity (ì˜ë¯¸ë¡ ì  ìœ ì‚¬ì„±): 0.8456
  - ë²”ìœ„: 0~1 (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
  - ìƒì„±ëœ ì„¤ëª…ì˜ ì˜ë¯¸ê°€ ê¸°ì¤€ê³¼ ì¼ì¹˜í•˜ëŠ” ì •ë„

âœ“ LLM Judge Score (ì„¤ëª… í’ˆì§ˆ): 0.8102
  - ë²”ìœ„: 0~1 (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
  - ì •í™•ì„±, ì™„ì „ì„±, ëª…í™•ì„±, êµ¬ì²´ì„±ì„ GPT-4oê°€ í‰ê°€

âœ“ Overall Score (ì¢…í•© ì ìˆ˜): 0.8173
  - BLEU 25% + Semantic 35% + LLM Judge 40%

================================================================================
í‰ê°€ëœ ìƒ˜í”Œ ìˆ˜: 100
================================================================================
```

## ğŸ’¾ ê²°ê³¼ ì €ì¥

í‰ê°€ ì™„ë£Œ í›„ ë‹¤ìŒ íŒŒì¼ì´ ìƒì„±ë©ë‹ˆë‹¤:

- **`./eval/results.json`**: ìƒì„¸ í‰ê°€ ê²°ê³¼
  - ì¢…í•© ì ìˆ˜ (summary)
  - ê° ìƒ˜í”Œë³„ ìƒì„¸ ì ìˆ˜ (details)
  - LLM Judge ì„¤ëª…

### results.json êµ¬ì¡°

```json
{
  "summary": {
    "bleu": 0.7234,
    "semantic_similarity": 0.8456,
    "llm_judge": 0.8102,
    "total_samples": 100
  },
  "details": [
    {
      "sample_id": 0,
      "reference": "ìŠ¤íŒ¸ íŒì • ì´ìœ : ...",
      "prediction": "ìƒì„±ëœ ì„¤ëª…: ...",
      "bleu_score": 0.75,
      "semantic_similarity": 0.82,
      "llm_judge_score": 0.88,
      "llm_judge_explanation": "ì •í™•í•˜ê³  êµ¬ì²´ì ì¸ ì„¤ëª…..."
    },
    ...
  ]
}
```

## ğŸ”§ ëª¨ë“ˆ êµ¬ì¡°

```
eval/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ evaluation.py          # ë©”ì¸ í‰ê°€ í´ë˜ìŠ¤
â””â”€â”€ README.md             # ì´ íŒŒì¼
```

### EvaluationMetrics í´ë˜ìŠ¤

ìƒ˜í”Œë³„ í‰ê°€ ë©”íŠ¸ë¦­ì„ ì €ì¥í•˜ëŠ” ë°ì´í„°í´ë˜ìŠ¤:

```python
@dataclass
class EvaluationMetrics:
    sample_id: int
    prompt: str
    reference: str
    prediction: str
    bleu_score: float
    semantic_similarity: float
    llm_judge_score: float
    llm_judge_explanation: str
```

### LLMEvaluation í´ë˜ìŠ¤

ì£¼ìš” ë©”ì„œë“œ:

| ë©”ì„œë“œ | ì„¤ëª… |
|--------|------|
| `__init__()` | í‰ê°€ê¸° ì´ˆê¸°í™” |
| `evaluate()` | ì „ì²´ í‰ê°€ ì‹¤í–‰ |
| `_calculate_semantic_similarity()` | ì˜ë¯¸ë¡ ì  ìœ ì‚¬ì„± ê³„ì‚° |
| `_llm_judge_evaluation()` | LLM Judge í‰ê°€ |
| `print_leaderboard()` | ë¦¬ë”ë³´ë“œ ì¶œë ¥ |
| `save_detailed_results()` | ê²°ê³¼ ì €ì¥ |

## ğŸ“¦ ì˜ì¡´ì„±

- `opik`: BLEU ë©”íŠ¸ë¦­ ê³„ì‚°
- `sentence-transformers`: Semantic Similarity ê³„ì‚°
- `openai`: LLM Judge (GPT-4o) í˜¸ì¶œ
- `datasets`: Spam_QA-Corpus ë¡œë“œ

## âš™ï¸ í™˜ê²½ ì„¤ì •

ë‹¤ìŒ í™˜ê²½ ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤:

```bash
# í‰ê°€ ëŒ€ìƒ ëª¨ë¸
SPAM_MODEL_URL="http://localhost:8000/v1"
SPAM_MODEL_API_KEY="your-api-key"
SPAM_MODEL="google/gemma-3-4b-it"

# LLM Judge (GPT-4o)
OPENAI_API_KEY="sk-..."
```

## ğŸ“ í‰ê°€ í•´ì„ ê°€ì´ë“œ

| ë²”ìœ„ | í•´ì„ | í‰ê°€ |
|------|------|------|
| 0.90 ~ 1.00 | ë§¤ìš° ìš°ìˆ˜ | â­â­â­â­â­ |
| 0.80 ~ 0.89 | ìš°ìˆ˜ | â­â­â­â­ |
| 0.70 ~ 0.79 | ì–‘í˜¸ | â­â­â­ |
| 0.60 ~ 0.69 | ë³´í†µ | â­â­ |
| 0.00 ~ 0.59 | ê°œì„  í•„ìš” | â­ |

## ğŸ”„ ëª¨ë¸ ë¹„êµ

ì—¬ëŸ¬ ëª¨ë¸ì„ ìˆœì°¨ì ìœ¼ë¡œ í‰ê°€í•˜ì—¬ ë¹„êµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```bash
# Model 1
python eval/evaluation.py --limit 100 --model "model1"

# Model 2
python eval/evaluation.py --limit 100 --model "model2"

# results.json ë¹„êµ
```

## ğŸ› ë¬¸ì œ í•´ê²°

### CrossEncoder ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜

```bash
python -m sentence_transformers.models download cross-encoder/qnli-distilroberta-base
```

### OpenAI API ì˜¤ë¥˜

```bash
# API í‚¤ í™•ì¸
echo $OPENAI_API_KEY

# ì—°ê²° í…ŒìŠ¤íŠ¸
python -c "from openai import OpenAI; print(OpenAI(api_key='sk-...').models.list())"
```

## ğŸ“š ì°¸ê³  ìë£Œ

- [BLEU Score](https://en.wikipedia.org/wiki/BLEU)
- [Semantic Similarity with Transformers](https://www.sbert.net/docs/usage/semantic_textual_similarity.html)
- [LLM as Judge](https://arxiv.org/abs/2306.05685)

## ğŸ¤ ê¸°ì—¬

ê°œì„  ì‚¬í•­ ë° ë²„ê·¸ ë¦¬í¬íŠ¸ëŠ” ì´ìŠˆë¡œ ë“±ë¡í•´ì£¼ì„¸ìš”!
