# ğŸ¯ ëª¨ë¸ ì–‘ìí™” ê°€ì´ë“œ

í•™ìŠµëœ LLM ëª¨ë¸ì„ AWQ ë˜ëŠ” GPTQë¡œ ì–‘ìí™”í•˜ì—¬ ì¶”ë¡  ì†ë„ë¥¼ ë†’ì´ê³  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ëŠ” ë„êµ¬ì…ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨

- [ê°œìš”](#ê°œìš”)
- [ì–‘ìí™” íƒ€ì… ë¹„êµ](#ì–‘ìí™”-íƒ€ì…-ë¹„êµ)
- [ì„¤ì¹˜](#ì„¤ì¹˜)
- [ì‚¬ìš©ë²•](#ì‚¬ìš©ë²•)
- [ê³ ê¸‰ ì„¤ì •](#ê³ ê¸‰-ì„¤ì •)
- [ë¬¸ì œ í•´ê²°](#ë¬¸ì œ-í•´ê²°)

## ğŸ¯ ê°œìš”

### ì–‘ìí™”ë€?

ì–‘ìí™”(Quantization)ëŠ” ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ì™€ í™œì„±í™” ê°’ì„ ë‚®ì€ ì •ë°€ë„(ì˜ˆ: 4bit)ë¡œ ë³€í™˜í•˜ì—¬ ëª¨ë¸ í¬ê¸°ë¥¼ ì¤„ì´ê³  ì¶”ë¡  ì†ë„ë¥¼ ë†’ì´ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.

**ì¥ì :**
- ğŸš€ **ì¶”ë¡  ì†ë„ í–¥ìƒ**: 2-4ë°° ë¹ ë¥¸ ì¶”ë¡ 
- ğŸ’¾ **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ**: ëª¨ë¸ í¬ê¸° 75% ê°ì†Œ (16bit â†’ 4bit)
- ğŸ’° **ë¹„ìš© ì ˆê°**: ë” ì‘ì€ GPUë¡œ ì„œë¹„ìŠ¤ ê°€ëŠ¥
- ğŸ“Š **ì •í™•ë„ ìœ ì§€**: ìµœì†Œí•œì˜ ì„±ëŠ¥ ì†ì‹¤ (1-3%)

## âš–ï¸ ì–‘ìí™” íƒ€ì… ë¹„êµ

### AWQ (Activation-aware Weight Quantization)

**íŠ¹ì§•:**
- âœ… **ë¹ ë¥¸ ì¶”ë¡  ì†ë„**: GPTQë³´ë‹¤ 1.5-2ë°° ë¹ ë¦„
- âœ… **ë†’ì€ ì •í™•ë„**: Activationì„ ê³ ë ¤í•œ ê°€ì¤‘ì¹˜ ì–‘ìí™”
- âœ… **ê°„ë‹¨í•œ ì„¤ì •**: ìë™ìœ¼ë¡œ ìµœì í™”
- âš ï¸ **ì œí•œì  ì§€ì›**: ì¼ë¶€ í•˜ë“œì›¨ì–´ì—ì„œë§Œ ì‘ë™

**ì¶”ì²œ ìƒí™©:**
- ìµœì‹  CUDA GPU (Ampere ì´ìƒ)
- ìµœëŒ€ ì„±ëŠ¥ì´ í•„ìš”í•œ ê²½ìš°
- Gemma, Llama ë“± ì£¼ìš” ëª¨ë¸

```bash
# AWQ ì–‘ìí™” (ê¶Œì¥)
./scripts/run_quantize.sh --quantization-type awq
```

### GPTQ (Generative Pre-trained Transformer Quantization)

**íŠ¹ì§•:**
- âœ… **ì•ˆì •ì **: ê´‘ë²”ìœ„í•œ í…ŒìŠ¤íŠ¸ì™€ ê²€ì¦
- âœ… **ë²”ìš©ì„±**: ëŒ€ë¶€ë¶„ì˜ GPUì—ì„œ ì‘ë™
- âœ… **ì»¤ë®¤ë‹ˆí‹° ì§€ì›**: ë§ì€ ë¬¸ì„œì™€ ì˜ˆì œ
- âš ï¸ **ëŠë¦° ì†ë„**: AWQë³´ë‹¤ ëŠë¦¼

**ì¶”ì²œ ìƒí™©:**
- êµ¬í˜• GPU (Pascal, Turing)
- AWQê°€ ì‘ë™í•˜ì§€ ì•Šì„ ë•Œ
- ì•ˆì •ì„±ì´ ì¤‘ìš”í•œ í”„ë¡œë•ì…˜ í™˜ê²½

```bash
# GPTQ ì–‘ìí™”
./scripts/run_quantize.sh --quantization-type gptq
```

### ë¹„êµ í‘œ

| í•­ëª© | AWQ | GPTQ |
|------|-----|------|
| ì¶”ë¡  ì†ë„ | âš¡âš¡âš¡ ë§¤ìš° ë¹ ë¦„ | âš¡âš¡ ë¹ ë¦„ |
| ì •í™•ë„ | ğŸ¯ ìš°ìˆ˜ | ğŸ¯ ì–‘í˜¸ |
| ë©”ëª¨ë¦¬ ì‚¬ìš© | ğŸ’¾ ë‚®ìŒ | ğŸ’¾ ë‚®ìŒ |
| GPU í˜¸í™˜ì„± | ğŸ”§ ì œí•œì  | ğŸ”§ ë²”ìš© |
| ì„¤ì • ë‚œì´ë„ | ğŸ˜Š ì‰¬ì›€ | ğŸ˜Š ì‰¬ì›€ |
| ì–‘ìí™” ì‹œê°„ | â±ï¸ ë¹ ë¦„ | â±ï¸ ì¤‘ê°„ |

## ğŸ”§ ì„¤ì¹˜

### 1. AWQ ì„¤ì¹˜

```bash
# AWQ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
pip install autoawq

# ë˜ëŠ” ìµœì‹  ë²„ì „ (GPU ì§€ì›)
pip install autoawq --extra-index-url https://wheels.autoawq.ai/
```

### 2. GPTQ ì„¤ì¹˜

```bash
# GPTQ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
pip install auto-gptq

# ë˜ëŠ” CUDA ë²„ì „ ì§€ì • (ì˜ˆ: CUDA 11.8)
pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/
```

### 3. ì˜ì¡´ì„± í™•ì¸

```bash
# PyTorch ë° transformers í™•ì¸
python -c "import torch; print(f'PyTorch: {torch.__version__}, CUDA: {torch.cuda.is_available()}')"
python -c "import transformers; print(f'Transformers: {transformers.__version__}')"
```

## ğŸš€ ì‚¬ìš©ë²•

### ê¸°ë³¸ ì›Œí¬í”Œë¡œìš°

```bash
# 1ë‹¨ê³„: ëª¨ë¸ í•™ìŠµ
./scripts/run_train.sh

# 2ë‹¨ê³„: LoRA ì–´ëŒ‘í„° ë³‘í•©
./scripts/run_merge.sh

# 3ë‹¨ê³„: ëª¨ë¸ ì–‘ìí™”
./scripts/run_quantize.sh
```

### ê¸°ë³¸ ì–‘ìí™” (AWQ)

```bash
# ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ AWQ ì–‘ìí™” (4bit, 128 group size)
./scripts/run_quantize.sh

# ë˜ëŠ” ì§ì ‘ Python ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
python -m src.optimizer.quantize \
  --model-path ./outputs/merged \
  --output-dir ./outputs/quantized/awq \
  --quantization-type awq
```

**ê²°ê³¼:**
- ì…ë ¥: `./outputs/merged` (ë³‘í•©ëœ ëª¨ë¸)
- ì¶œë ¥: `./outputs/quantized/awq` (4bit AWQ ì–‘ìí™” ëª¨ë¸)

### GPTQ ì–‘ìí™”

```bash
# GPTQë¡œ ì–‘ìí™”
./scripts/run_quantize.sh --quantization-type gptq

# ì¶œë ¥ ê²½ë¡œ ì§€ì •
./scripts/run_quantize.sh \
  --quantization-type gptq \
  --output-dir ./outputs/quantized_models
```

### 8bit ì–‘ìí™”

```bash
# ë” ë†’ì€ ì •í™•ë„ê°€ í•„ìš”í•œ ê²½ìš° 8bit ì–‘ìí™”
./scripts/run_quantize.sh --bits 8

# ë˜ëŠ” GPTQ 8bit
./scripts/run_quantize.sh --quantization-type gptq --bits 8
```

**ë¹„íŠ¸ ìˆ˜ë³„ íŠ¹ì§•:**
- **4bit**: ìµœëŒ€ ì••ì¶•, ë¹ ë¥¸ ì†ë„, ì•½ê°„ì˜ ì •í™•ë„ ì†ì‹¤
- **8bit**: ê· í˜•ì¡íŒ ì„±ëŠ¥, ìµœì†Œí•œì˜ ì •í™•ë„ ì†ì‹¤

### Calibration ìƒ˜í”Œ ìˆ˜ ì¡°ì •

```bash
# ë” ë§ì€ ìƒ˜í”Œë¡œ ì •í™•ë„ í–¥ìƒ (ì‹œê°„ ì¦ê°€)
./scripts/run_quantize.sh --calibration-samples 1024

# ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš© (ì ì€ ìƒ˜í”Œ)
./scripts/run_quantize.sh --calibration-samples 128
```

**ê¶Œì¥ ìƒ˜í”Œ ìˆ˜:**
- **ë¹ ë¥¸ í…ŒìŠ¤íŠ¸**: 128 ìƒ˜í”Œ
- **ì¼ë°˜ ì‚¬ìš©**: 512 ìƒ˜í”Œ (ê¸°ë³¸ê°’)
- **ë†’ì€ ì •í™•ë„**: 1024-2048 ìƒ˜í”Œ

## ğŸ›ï¸ ê³ ê¸‰ ì„¤ì •

### ì „ì²´ ì˜µì…˜

```bash
./scripts/run_quantize.sh \
  --model-path ./outputs/merged \
  --output-dir ./outputs/quantized \
  --quantization-type awq \
  --bits 4 \
  --group-size 128 \
  --calibration-samples 512 \
  --dataset-name Devocean-06/Spam_QA-Corpus \
  --dataset-split train \
  --max-seq-length 1500
```

### íŒŒë¼ë¯¸í„° ì„¤ëª…

| íŒŒë¼ë¯¸í„° | ì„¤ëª… | ê¸°ë³¸ê°’ | ê¶Œì¥ê°’ |
|---------|------|--------|--------|
| `--model-path` | ì–‘ìí™”í•  ëª¨ë¸ ê²½ë¡œ | `./outputs/merged` | - |
| `--output-dir` | ì €ì¥ ê²½ë¡œ | `./outputs/quantized` | - |
| `--quantization-type` | ì–‘ìí™” íƒ€ì… | `awq` | `awq` (ë¹ ë¦„), `gptq` (ì•ˆì •) |
| `--bits` | ì–‘ìí™” ë¹„íŠ¸ | `4` | `4` (ì••ì¶•), `8` (ì •í™•ë„) |
| `--group-size` | ê·¸ë£¹ ì‚¬ì´ì¦ˆ | `128` | `128` (ì¼ë°˜), `64` (ì •í™•ë„) |
| `--calibration-samples` | Calibration ìƒ˜í”Œ ìˆ˜ | `512` | `512-1024` |
| `--max-seq-length` | ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ | `1500` | ëª¨ë¸ ì„¤ì •ê³¼ ë™ì¼ |

### ë‹¤ì–‘í•œ ì„¤ì • ì˜ˆì‹œ

#### 1. ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘

```bash
# ì ì€ ìƒ˜í”Œ, ë¹ ë¥¸ ì–‘ìí™”
./scripts/run_quantize.sh \
  --calibration-samples 128 \
  --bits 4
```

#### 2. í”„ë¡œë•ì…˜ ë°°í¬

```bash
# ë†’ì€ ì •í™•ë„, ìµœì í™”ëœ ì„¤ì •
./scripts/run_quantize.sh \
  --quantization-type awq \
  --bits 4 \
  --group-size 128 \
  --calibration-samples 1024
```

#### 3. ë©”ëª¨ë¦¬ ì œì•½ í™˜ê²½

```bash
# ìµœëŒ€ ì••ì¶•
./scripts/run_quantize.sh \
  --bits 4 \
  --calibration-samples 256
```

#### 4. ì •í™•ë„ ìš°ì„ 

```bash
# 8bit ì–‘ìí™”, ë§ì€ calibration ìƒ˜í”Œ
./scripts/run_quantize.sh \
  --bits 8 \
  --group-size 64 \
  --calibration-samples 2048
```

## ğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

### Gemma-3-4B ëª¨ë¸ ê¸°ì¤€

| ì„¤ì • | ëª¨ë¸ í¬ê¸° | ì¶”ë¡  ì†ë„ | ì •í™•ë„ | ë©”ëª¨ë¦¬ |
|------|----------|----------|--------|--------|
| FP16 (ì›ë³¸) | ~8GB | 1x | 100% | ~10GB |
| AWQ 4bit | ~2GB | 3.5x | 98% | ~3GB |
| GPTQ 4bit | ~2GB | 2.5x | 97% | ~3GB |
| AWQ 8bit | ~4GB | 2x | 99% | ~5GB |

**ì¸¡ì • í™˜ê²½**: NVIDIA A100, Batch Size 1, Seq Length 512

## ğŸ” ì–‘ìí™”ëœ ëª¨ë¸ ì‚¬ìš©

### 1. Pythonì—ì„œ ë¡œë“œ

#### AWQ ëª¨ë¸

```python
from awq import AutoAWQForCausalLM
from transformers import AutoTokenizer

# ì–‘ìí™”ëœ ëª¨ë¸ ë¡œë“œ
model = AutoAWQForCausalLM.from_quantized(
    "./outputs/quantized/awq",
    fuse_layers=True,
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("./outputs/quantized/awq")

# ì¶”ë¡ 
inputs = tokenizer("ìŠ¤íŒ¸ ë¬¸ìë¥¼ íŒì •í•´ì£¼ì„¸ìš”", return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, max_new_tokens=100)
print(tokenizer.decode(outputs[0]))
```

#### GPTQ ëª¨ë¸

```python
from auto_gptq import AutoGPTQForCausalLM
from transformers import AutoTokenizer

# ì–‘ìí™”ëœ ëª¨ë¸ ë¡œë“œ
model = AutoGPTQForCausalLM.from_quantized(
    "./outputs/quantized/gptq",
    device_map="auto",
    use_safetensors=True
)
tokenizer = AutoTokenizer.from_pretrained("./outputs/quantized/gptq")

# ì¶”ë¡ 
inputs = tokenizer("ìŠ¤íŒ¸ ë¬¸ìë¥¼ íŒì •í•´ì£¼ì„¸ìš”", return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, max_new_tokens=100)
print(tokenizer.decode(outputs[0]))
```

### 2. vLLM ì„œë²„ë¡œ ë°°í¬

```bash
# AWQ ëª¨ë¸ë¡œ vLLM ì„œë²„ ì‹œì‘
python -m vllm.entrypoints.openai.api_server \
  --model ./outputs/quantized/awq \
  --quantization awq \
  --dtype auto \
  --api-key token-abc123 \
  --port 8000

# GPTQ ëª¨ë¸ë¡œ vLLM ì„œë²„ ì‹œì‘
python -m vllm.entrypoints.openai.api_server \
  --model ./outputs/quantized/gptq \
  --quantization gptq \
  --dtype auto \
  --api-key token-abc123 \
  --port 8000
```

### 3. HuggingFace Hubì— ì—…ë¡œë“œ

```python
from huggingface_hub import HfApi

# ì–‘ìí™”ëœ ëª¨ë¸ ì—…ë¡œë“œ
api = HfApi()
api.upload_folder(
    folder_path="./outputs/quantized/awq",
    repo_id="your-username/gemma-3-4b-spam-awq",
    repo_type="model",
)
```

## â“ ë¬¸ì œ í•´ê²°

### AWQ ì„¤ì¹˜ ì˜¤ë¥˜

**ë¬¸ì œ**: `ImportError: cannot import name 'AutoAWQForCausalLM'`

**í•´ê²°**:
```bash
pip uninstall autoawq -y
pip install autoawq --extra-index-url https://wheels.autoawq.ai/
```

### CUDA Out of Memory

**ë¬¸ì œ**: ì–‘ìí™” ì¤‘ CUDA OOM ì—ëŸ¬

**í•´ê²°**:
```bash
# Calibration ìƒ˜í”Œ ìˆ˜ ì¤„ì´ê¸°
./scripts/run_quantize.sh --calibration-samples 256

# ë˜ëŠ” ì‹œí€€ìŠ¤ ê¸¸ì´ ì¤„ì´ê¸°
./scripts/run_quantize.sh --max-seq-length 1024
```

### GPTQ ì–‘ìí™” ëŠë¦¼

**ë¬¸ì œ**: GPTQ ì–‘ìí™”ê°€ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦¼

**í•´ê²°**:
```bash
# Calibration ìƒ˜í”Œ ìˆ˜ ì¤„ì´ê¸°
./scripts/run_quantize.sh \
  --quantization-type gptq \
  --calibration-samples 256

# ë˜ëŠ” AWQ ì‚¬ìš©
./scripts/run_quantize.sh --quantization-type awq
```

### ì •í™•ë„ í•˜ë½

**ë¬¸ì œ**: ì–‘ìí™” í›„ ì„±ëŠ¥ ì €í•˜

**í•´ê²°**:
```bash
# 8bit ì–‘ìí™” ì‹œë„
./scripts/run_quantize.sh --bits 8

# Calibration ìƒ˜í”Œ ì¦ê°€
./scripts/run_quantize.sh --calibration-samples 2048

# Group size ê°ì†Œ (ë” ì •í™•í•œ ì–‘ìí™”)
./scripts/run_quantize.sh --group-size 64
```

### ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨

**ë¬¸ì œ**: ì–‘ìí™”ëœ ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŒ

**í•´ê²°**:
```python
# AWQ: fuse_layers ë¹„í™œì„±í™”
model = AutoAWQForCausalLM.from_quantized(
    model_path,
    fuse_layers=False
)

# GPTQ: inject_fused_attention ë¹„í™œì„±í™”
model = AutoGPTQForCausalLM.from_quantized(
    model_path,
    inject_fused_attention=False
)
```

## ğŸ“ ì¶”ê°€ ìë£Œ

### ì°¸ê³  ë¬¸ì„œ

- [AWQ ë…¼ë¬¸](https://arxiv.org/abs/2306.00978)
- [GPTQ ë…¼ë¬¸](https://arxiv.org/abs/2210.17323)
- [AutoAWQ GitHub](https://github.com/casper-hansen/AutoAWQ)
- [AutoGPTQ GitHub](https://github.com/PanQiWei/AutoGPTQ)

### ê´€ë ¨ ìŠ¤í¬ë¦½íŠ¸

- `scripts/run_train.sh`: ëª¨ë¸ í•™ìŠµ
- `scripts/run_merge.sh`: LoRA ì–´ëŒ‘í„° ë³‘í•©
- `scripts/run_vllm_server.sh`: vLLM ì„œë²„ ì‹¤í–‰
- `eval/evaluation.py`: ëª¨ë¸ í‰ê°€

## ğŸ¤ ê¸°ì—¬

ì–‘ìí™” ê´€ë ¨ ê°œì„  ì‚¬í•­ì´ë‚˜ ë²„ê·¸ ë¦¬í¬íŠ¸ëŠ” ì´ìŠˆë¡œ ë“±ë¡í•´ì£¼ì„¸ìš”!

---

**Made with ğŸ± by Skitty Team**

