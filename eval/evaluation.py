import asyncio
import json
import logging
from dataclasses import dataclass
from typing import Optional
from datasets import load_dataset
from openai import AsyncOpenAI
from opik.evaluation.metrics import SentenceBLEU
from sentence_transformers import CrossEncoder
import numpy as np
from src.config.env_config import get_config
from src.utils.log import logger, async_log_performance, async_decorator_log

env = get_config()


def safe_mean(values: list[float]) -> float:
    """ë¹ˆ ë¦¬ìŠ¤íŠ¸ì¼ ê²½ìš° 0.0 ë°˜í™˜"""
    return sum(values) / len(values) if values else 0.0


@dataclass
class EvaluationMetrics:
    """í‰ê°€ ë©”íŠ¸ë¦­ ë°ì´í„° í´ë˜ìŠ¤"""
    sample_id: int
    prompt: str
    reference: str
    prediction: str
    bleu_score: float
    semantic_similarity: float
    llm_judge_score: float
    llm_judge_explanation: str


class LLMEvaluation:
    def __init__(self, limit: int | None = None, model_name: str = "google/gemma-3-4b-it"):
        self.dataset = load_dataset("Devocean-06/Spam_QA-Corpus", split="test")

        if limit:
            self.dataset = self.dataset.select(range(min(limit, len(self.dataset))))

        # ë©”íŠ¸ë¦­ ì´ˆê¸°í™”
        self.bleu = SentenceBLEU()
        # Semantic Similarityìš© CrossEncoder (ìŠ¤íŒ¸ ì„¤ëª… í’ˆì§ˆ í‰ê°€)
        self.semantic_model = CrossEncoder("cross-encoder/qnli-distilroberta-base")

        # í‰ê°€ ëŒ€ìƒ ëª¨ë¸
        self.model = AsyncOpenAI(base_url=env.SPAM_MODEL_URL, api_key=env.SPAM_MODEL_API_KEY)
        self.model_name = model_name

        # LLM Judge (SOTA ëª¨ë¸ - GPT-4o)
        self.judge_model = AsyncOpenAI(api_key=env.OPENAI_API_KEY)
        self.judge_model_name = "gpt-4o"

    def _build_prompt(self, sample: dict) -> tuple[str, str]:
        """Alpaca í¬ë§·(instruction/input/output) â†’ í”„ë¡¬í”„íŠ¸/ì •ë‹µ ìƒì„±"""
        instruction = sample["instruction"].strip()
        input_text = sample["input"].strip()
        reference = sample["output"].strip()

        if input_text:
            prompt = f"{instruction}\n\nInput: {input_text}"
        else:
            prompt = instruction

        return prompt, reference

    def _calculate_semantic_similarity(self, reference: str, prediction: str) -> float:
        """
        CrossEncoderë¥¼ ì‚¬ìš©í•œ ì˜ë¯¸ë¡ ì  ìœ ì‚¬ì„± ê³„ì‚°
        ì„¤ëª…(reference)ì´ ìƒì„±ëœ ì„¤ëª…(prediction)ê³¼ ì–¼ë§ˆë‚˜ ì¼ì¹˜í•˜ëŠ”ì§€ ì¸¡ì •
        """
        try:
            scores = self.semantic_model.predict([[reference, prediction]])
            # ì ìˆ˜ë¥¼ 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
            similarity = float((scores[0] + 1) / 2)  # -1~1 â†’ 0~1
            return min(max(similarity, 0.0), 1.0)
        except Exception as e:
            logger.warning("ì˜ë¯¸ë¡ ì  ìœ ì‚¬ì„± ê³„ì‚° ì‹¤íŒ¨: %s", e)
            return 0.0

    @async_decorator_log(level=logging.DEBUG)
    @async_log_performance
    async def _llm_judge_evaluation(
        self, reference: str, prediction: str, spam_text: str
    ) -> tuple[float, str]:
        """
        LLM as Judge: GPT-4oê°€ XAI ì„¤ëª…ì˜ í’ˆì§ˆì„ í‰ê°€
        - ì •í™•ì„±: ì„¤ëª…ì´ ìŠ¤íŒ¸ íŒì •ê³¼ ì¼ì¹˜í•˜ëŠ”ê°€?
        - ì™„ì „ì„±: ëª¨ë“  ê·¼ê±°ë¥¼ í¬í•¨í–ˆëŠ”ê°€?
        - ëª…í™•ì„±: ì„¤ëª…ì´ ëª…í™•í•œê°€?
        """
        judge_prompt = f"""ë‹¹ì‹ ì€ ìŠ¤íŒ¸ íƒì§€ ì„¤ëª… í’ˆì§ˆ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

[ìŠ¤íŒ¸ í…ìŠ¤íŠ¸]
{spam_text}

[ê¸°ì¤€ ì„¤ëª… (ë ˆí¼ëŸ°ìŠ¤)]
{reference}

[ìƒì„±ëœ ì„¤ëª… (í‰ê°€ ëŒ€ìƒ)]
{prediction}

ìœ„ ìƒì„±ëœ ì„¤ëª…ì´ ì–¼ë§ˆë‚˜ ì¢‹ì€ì§€ í‰ê°€í•´ì£¼ì„¸ìš”.

í‰ê°€ ê¸°ì¤€:
- ì •í™•ì„± (Accuracy): ì„¤ëª…ì´ ìŠ¤íŒ¸ íŒì •ì˜ ì‹¤ì œ ì´ìœ ì™€ ì¼ì¹˜í•˜ëŠ”ê°€?
- ì™„ì „ì„± (Completeness): ëª¨ë“  ìŠ¤íŒ¸ íŒì • ê·¼ê±°ë¥¼ í¬í•¨í–ˆëŠ”ê°€?
- ëª…í™•ì„± (Clarity): ì„¤ëª…ì´ ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ê°€?
- êµ¬ì²´ì„± (Specificity): êµ¬ì²´ì ì¸ ê·¼ê±°ë¥¼ ì œì‹œí–ˆëŠ”ê°€?

ì‘ë‹µ í˜•ì‹:
{{
  "score": <0.0~1.0 ì‚¬ì´ì˜ ì ìˆ˜>,
  "explanation": "<í‰ê°€ ì´ìœ ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ>"
}}"""

        try:
            resp = await self.judge_model.chat.completions.create(
                model=self.judge_model_name,
                messages=[{"role": "user", "content": judge_prompt}],
                temperature=0.1,
            )
            
            response_text = resp.choices[0].message.content.strip()
            
            # JSON íŒŒì‹±
            try:
                result = json.loads(response_text)
                score = float(result.get("score", 0.0))
                explanation = str(result.get("explanation", "í‰ê°€ ì‹¤íŒ¨"))
                return min(max(score, 0.0), 1.0), explanation
            except json.JSONDecodeError:
                logger.warning("LLM Judge ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: %s", response_text)
                return 0.0, "JSON íŒŒì‹± ì‹¤íŒ¨"

        except Exception as e:
            logger.warning("LLM Judge í‰ê°€ ì‹¤íŒ¨: %s", e)
            return 0.0, f"í‰ê°€ ì¤‘ ì˜¤ë¥˜: {str(e)}"

    @async_decorator_log(level=logging.DEBUG)
    @async_log_performance
    async def evaluate(self, verbose: bool = False):
        """
        ì „ì²´ í‰ê°€ ì‹¤í–‰: BLEU + Semantic Similarity + LLM as Judge
        """
        metrics_list: list[EvaluationMetrics] = []
        bleu_scores: list[float] = []
        semantic_scores: list[float] = []
        llm_judge_scores: list[float] = []

        for i, sample in enumerate(self.dataset):
            try:
                prompt, reference = self._build_prompt(sample)
                spam_text = sample.get("input", "")

                # ëª¨ë¸ ì¶”ë¡ 
                resp = await self.model.chat.completions.create(
                    model=self.model_name,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.0,
                )
                prediction = resp.choices[0].message.content.strip()

                # 1. BLEU ì ìˆ˜ ê³„ì‚°
                bleu = self.bleu.score(output=prediction, references=[reference])
                bleu_score = float(bleu)
                bleu_scores.append(bleu_score)

                # 2. Semantic Similarity ê³„ì‚°
                semantic_sim = self._calculate_semantic_similarity(reference, prediction)
                semantic_scores.append(semantic_sim)

                # 3. LLM as Judge í‰ê°€
                judge_score, judge_explanation = await self._llm_judge_evaluation(
                    reference, prediction, spam_text
                )
                llm_judge_scores.append(judge_score)

                # ë©”íŠ¸ë¦­ ì €ì¥
                metrics = EvaluationMetrics(
                    sample_id=i,
                    prompt=prompt,
                    reference=reference,
                    prediction=prediction,
                    bleu_score=bleu_score,
                    semantic_similarity=semantic_sim,
                    llm_judge_score=judge_score,
                    llm_judge_explanation=judge_explanation,
                )
                metrics_list.append(metrics)

                if verbose and i % 10 == 0:
                    logger.info(
                        f"[{i}/{len(self.dataset)}] BLEU: {bleu_score:.4f}, "
                        f"Semantic: {semantic_sim:.4f}, Judge: {judge_score:.4f}"
                    )

            except Exception as e:
                logger.warning("ìƒ˜í”Œ %d ì²˜ë¦¬ ìŠ¤í‚µ: %s", i, e)

        if not bleu_scores:
            raise RuntimeError("ìœ íš¨í•œ ìƒ˜í”Œì´ ì—†ì–´ ì ìˆ˜ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # í‰ê°€ ê²°ê³¼ ê³„ì‚°
        results = {
            "bleu": safe_mean(bleu_scores),
            "semantic_similarity": safe_mean(semantic_scores),
            "llm_judge": safe_mean(llm_judge_scores),
            "metrics_details": metrics_list,
            "total_samples": len(metrics_list),
        }

        return results

    def print_leaderboard(self, results: dict):
        """ë¦¬ë”ë³´ë“œ í˜•íƒœë¡œ ê²°ê³¼ ì¶œë ¥"""
        print("\n" + "=" * 80)
        print("ğŸ† SPAM XAI EVALUATION LEADERBOARD ğŸ†".center(80))
        print("=" * 80 + "\n")

        # ì¢…í•© ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ í‰ê· )
        weights = {"bleu": 0.25, "semantic_similarity": 0.35, "llm_judge": 0.40}
        overall_score = (
            results["bleu"] * weights["bleu"]
            + results["semantic_similarity"] * weights["semantic_similarity"]
            + results["llm_judge"] * weights["llm_judge"]
        )

        # ë©”ì¸ ì ìˆ˜ í‘œì‹œ
        print(f"{'Model':<20} {'BLEU':<12} {'Semantic':<12} {'LLM Judge':<12} {'Overall':<12}")
        print("-" * 68)
        print(
            f"{self.model_name:<20} {results['bleu']:<12.4f} "
            f"{results['semantic_similarity']:<12.4f} {results['llm_judge']:<12.4f} "
            f"{overall_score:<12.4f}"
        )

        print("\n" + "-" * 80)
        print("ğŸ“Š Metric Details".center(80))
        print("-" * 80)

        print(f"\nâœ“ BLEU Score (ì–´íœ˜ ì¼ì¹˜ë„): {results['bleu']:.4f}")
        print(f"  - ë²”ìœ„: 0~1 (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)")
        print(f"  - ì„¤ëª…ì´ ê¸°ì¤€ ì„¤ëª…ê³¼ ì–¼ë§ˆë‚˜ ì¼ì¹˜í•˜ëŠ”ì§€ ì¸¡ì •\n")

        print(f"âœ“ Semantic Similarity (ì˜ë¯¸ë¡ ì  ìœ ì‚¬ì„±): {results['semantic_similarity']:.4f}")
        print(f"  - ë²”ìœ„: 0~1 (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)")
        print(f"  - ìƒì„±ëœ ì„¤ëª…ì˜ ì˜ë¯¸ê°€ ê¸°ì¤€ê³¼ ì¼ì¹˜í•˜ëŠ” ì •ë„\n")

        print(f"âœ“ LLM Judge Score (ì„¤ëª… í’ˆì§ˆ): {results['llm_judge']:.4f}")
        print(f"  - ë²”ìœ„: 0~1 (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)")
        print(f"  - ì •í™•ì„±, ì™„ì „ì„±, ëª…í™•ì„±, êµ¬ì²´ì„±ì„ GPT-4oê°€ í‰ê°€\n")

        print(f"âœ“ Overall Score (ì¢…í•© ì ìˆ˜): {overall_score:.4f}")
        print(f"  - BLEU 25% + Semantic 35% + LLM Judge 40%\n")

        print("=" * 80)
        print(f"í‰ê°€ëœ ìƒ˜í”Œ ìˆ˜: {results['total_samples']}")
        print("=" * 80 + "\n")

    def save_detailed_results(self, results: dict, output_path: str = "./eval/results.json"):
        """ìƒì„¸ í‰ê°€ ê²°ê³¼ ì €ì¥"""
        export_data = {
            "summary": {
                "bleu": results["bleu"],
                "semantic_similarity": results["semantic_similarity"],
                "llm_judge": results["llm_judge"],
                "total_samples": results["total_samples"],
            },
            "details": [
                {
                    "sample_id": m.sample_id,
                    "reference": m.reference,
                    "prediction": m.prediction,
                    "bleu_score": m.bleu_score,
                    "semantic_similarity": m.semantic_similarity,
                    "llm_judge_score": m.llm_judge_score,
                    "llm_judge_explanation": m.llm_judge_explanation,
                }
                for m in results["metrics_details"]
            ],
        }

        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)

        logger.info(f"ìƒì„¸ ê²°ê³¼ ì €ì¥: {output_path}")


if __name__ == "__main__":
    import sys
    import argparse
    
    async def main():
        """í‰ê°€ ì‹¤í–‰ì„ ìœ„í•œ ë©”ì¸ í•¨ìˆ˜"""
        # CLI ì¸ìˆ˜ íŒŒì‹±
        parser = argparse.ArgumentParser(description="LLM XAI í‰ê°€ ìŠ¤í¬ë¦½íŠ¸")
        parser.add_argument("--limit", type=int, default=None, help="í‰ê°€í•  ìƒ˜í”Œ ìˆ˜ ì œí•œ")
        parser.add_argument("--verbose", type=lambda x: x.lower() == "true", default=False, help="ìƒì„¸ ë¡œê·¸ ì¶œë ¥")
        parser.add_argument("--model", type=str, default="google/gemma-3-4b-it", help="í‰ê°€í•  ëª¨ë¸ëª…")
        
        args = parser.parse_args()
        
        logger.info("ğŸš€ LLM XAI í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        logger.info(f"ğŸ“Š ì„¤ì •: limit={args.limit}, verbose={args.verbose}, model={args.model}")

        # í‰ê°€ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        evaluator = LLMEvaluation(limit=args.limit, model_name=args.model)

        try:
            results = await evaluator.evaluate(verbose=args.verbose)

            # ë¦¬ë”ë³´ë“œ ì¶œë ¥
            evaluator.print_leaderboard(results)

            # ìƒì„¸ ê²°ê³¼ ì €ì¥
            evaluator.save_detailed_results(results)

        except Exception as e:
            logger.error("í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: %s", e)
            raise

    # ë¹„ë™ê¸° ë©”ì¸ í•¨ìˆ˜ ì‹¤í–‰
    asyncio.run(main())